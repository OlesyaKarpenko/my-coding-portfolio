{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3774dd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, io, nltk, re\n",
    "from collections import Counter \n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d00db172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "# директория, где располагается снимок википедии\n",
    "enwikipath = \"C:/Users/monte/_Olesya/Диплом/Основная папка/задание/enwiki20201020\"\n",
    "\n",
    "# в файле catalog.json для каждой статьи категории NLP хранится имя файла из снимка википедии\n",
    "cat = json.load(open(\"../catalog.json\", \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "model = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bf1e81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# выполняется поиск статьи в каталоге\n",
    "def FindArticle(j, art_title):\n",
    "    for x in j:\n",
    "        if x['title'] == art_title:\n",
    "            return x\n",
    "    return None\n",
    "\n",
    "\n",
    "def my_comp_distance(x):\n",
    "    return x.distance\n",
    "\n",
    "@dataclass\n",
    "class dist_to_root_art:\n",
    "    idx: int\n",
    "    art_name: str    \n",
    "    distance: float\n",
    "    \n",
    "@dataclass\n",
    "class art_dist:\n",
    "    idx: int\n",
    "    art_name: str  \n",
    "    distances: list[dist_to_root_art]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a759e6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vector(text, model):\n",
    "    # Разбиваем текст на слова\n",
    "    words = text.split()\n",
    "    \n",
    "    # Создаем список векторов для всех слов в тексте\n",
    "    word_vectors = []\n",
    "    for word in words:\n",
    "        if word in model.key_to_index:  # Проверяем, есть ли слово в модели\n",
    "            word_vectors.append(model[word])\n",
    "    \n",
    "    # Усредняем векторы слов для получения единого вектора текста\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  # Возвращаем нулевой вектор, если слов нет в модели\n",
    "\n",
    "def calculate_distance(text1, text2, model):\n",
    "    # Получаем векторы для двух текстов\n",
    "    vector1 = text_to_vector(text1, model)\n",
    "    vector2 = text_to_vector(text2, model)\n",
    "    \n",
    "    # Вычисляем косинусное расстояние между векторами\n",
    "    return cosine(vector1, vector2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2091fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [=====================] 100.0000%\r"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "NLP_Pages = json.load(open(\"../NatLangProc_Pages.json\", \"r\", encoding=\"utf-8\"))\n",
    "NLP_P = list()\n",
    "for x in NLP_Pages:\n",
    "    if NLP_Pages[x][0] == \"Natural language processing\":\n",
    "        NLP_P.append(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "articles = []\n",
    "\n",
    "for i in range(len(NLP_P)):\n",
    "    if (NLP_P[i] in cat):\n",
    "        fil_i = cat[NLP_P[i]]\n",
    "        j_i = json.load(open(enwikipath+\"/\"+fil_i, \"r\", encoding=\"utf-8\"))\n",
    "        art_i = FindArticle(j_i, NLP_P[i])\n",
    "        text_i = art_i['text']\n",
    "        \n",
    "        \n",
    "        article = art_dist(i,NLP_P[i],[])\n",
    "        \n",
    "        for k in range(i, len(NLP_P)):\n",
    "            if (NLP_P[k] in cat):\n",
    "                fil_k = cat[NLP_P[k]]\n",
    "                j_k = json.load(open(enwikipath+\"/\"+fil_k, \"r\", encoding=\"utf-8\"))\n",
    "                art_k = FindArticle(j_k, NLP_P[k])\n",
    "                text_k = art_k['text']\n",
    "               \n",
    "                \n",
    "                article.distances.append(dist_to_root_art(k, NLP_P[k],calculate_distance(text1, text2, model)))\n",
    "                \n",
    "            progress = (len(NLP_P)*(i) + k + 1)/(len(NLP_P)**2)\n",
    "            print(f\"Progress: [{'='*int(progress*20)}{'>' if progress < 1 else '='}{'-'*int((1-progress)*20)}] {progress*100:3.4f}%\", end = '\\r')\n",
    "        \n",
    "        articles.append(article)\n",
    "\n",
    "        \n",
    "for i in range (len(articles)):\n",
    "    for j in range(i + 1, len(articles)):\n",
    "        for k in articles[i].distances:\n",
    "            if k.idx == articles[j].idx:\n",
    "                articles[j].distances.append(dist_to_root_art(articles[i].idx, articles[i].art_name, k.distance))\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d906c42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_dict = {}\n",
    "for i in range (len(articles)):\n",
    "    idx = articles[i].idx\n",
    "    art_name = articles[i].art_name\n",
    "    distances = {}\n",
    "    for k in articles[i].distances:\n",
    "        distances[k.idx] = {\"art_name\": k.art_name, \"distance\": k.distance}\n",
    "    articles_dict[idx] = {\"art_name\": art_name, \"distances\": distances}\n",
    "    \n",
    "with open('NLP_distances_result_word2vec.json', 'w') as file:\n",
    "    json.dump(articles_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b42f2ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_art_sum (name):\n",
    "    art_dict = json.load(open(path_to_folder + \"/\" + name + \".json\", \"r\", encoding=\"utf-8\"))\n",
    "    sum_list = []\n",
    "    for i in art_dict:\n",
    "        sum_el = 0\n",
    "        for j in art_dict[i][\"distances\"]:\n",
    "            sum_el += art_dict[i][\"distances\"][j][\"distance\"]\n",
    "        sum_list.append(dist_to_root_art(i, art_dict[i][\"art_name\"], sum_el ** (1 / 2)))\n",
    "    return max(sum_list, key = my_comp_distance).art_name\n",
    "\n",
    "def main_art (name):\n",
    "    return categ[name][3]\n",
    "\n",
    "def print_cat_main_closest (name, name_json):\n",
    "    print(\"Category name: \" + name + \"\\n\")\n",
    "    print(\"Main articles: \")\n",
    "    list_art = ()\n",
    "    list_art = main_art(name)\n",
    "    for i in range(len(list_art)):\n",
    "        print(list_art[i] + \", \")\n",
    "    print(\"\\n\" +  \"Closest article: \" + closest_art_sum(name_json))\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3819e9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category name: Natural language processing\n",
      "\n",
      "Main articles: \n",
      "Natural language processing, \n",
      "\n",
      "Closest article: Natural language processing\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "categ = json.load(open(\"C:/Users/monte/_Olesya/Диплом/Основная папка/задание/NatLangProc_Categ.json\", \"r\", encoding=\"utf-8\"))\n",
    "path_to_folder = \"C:/Users/monte/_Olesya/Диплом/Основная папка/задание/Word2vec\"\n",
    "print_cat_main_closest(\"Natural language processing\", \"NLP_distances_result_word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490e457b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
